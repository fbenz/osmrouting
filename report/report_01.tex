\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx}  % import graphics
\usepackage{listings}  % support source code listing
\usepackage{amsmath}  % math stuff
\usepackage{amssymb} % 
\usepackage{a4wide} % wide pages
\usepackage{fancyhdr} % nice headers
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{petri}

\lstset{basicstyle=\footnotesize,language=Python,numbers=left, numberstyle=\tiny, stepnumber=5,firstnumber=0, numbersep=5pt} % set up listings
\pagestyle{fancy}             % header

\usepackage[none]{hyphenat}
\sloppy

% change enums style: first level (a), (b), (c)           
\renewcommand{\labelenumi}{\arabic{enumi}.}
\renewcommand{\labelenumii}{\alph{enumii})}

%term/semester
\newcommand{\term}{
	Summer 2012 -- Saarland University
} 

%lecture name
\newcommand{\lecture}{
	Algorithm Engineering
}           

%assignment iteration
\newcommand{\assignment}{
	Team FortyTwo Report
}

%set up names, matricle number, and email
\newcommand{\authors}{
  Florian Benz, Steven Schäfer, Bernhard Schommer
}

% use to start a new exercise
\newcommand{\exercise}[1]
{
  \stepcounter{subsection}
  \subsection*{Exercise \thesubsection: #1}

}

\begin{document}
\title{
  \vspace{-3cm}
  \large
  \lecture \\
  \small \term \\
  \huge \assignment
}
\author{\small \authors}

\setlength \headheight{25pt}
\fancyhead[R]{\begin{tabular}{r}\lecture \\ \assignment \end{tabular}}
\fancyhead[L]{\authors}

\maketitle

\section{Overview}
First, we pre-process the OSM data to extract a street graph consisting of intersections in the original graph.
For each travel mode we currently emit a separate graph containing only those vertices and edges relevant for the travel mode.
Our graph format consists of a compressed sparse row representation of the graph.
Each vertex and edge can be uniquely identified by its offset.
We use this to store additional attributes like distance, steps and positions in separate files.

During the startup of the server, all files are mapped into the server's address space.
As a result our startup overhead is negligible and we do not have to load data from disk afterwards.

When a query arrives, we find for each waypoint the nearest node in the OSM data.
This might not correspond to a vertex in our street graph;
in this case the node lies on an edge and we compute all reachable endpoints of this edge.
At this point, we run Dijkstra's algorithm on the street graph.
Finally, the resulting JSON object is created and returned.

The street graph is significantly smaller than the original OSM graph.
Furthermore, it is sparse; the average out-degree is around 2.4.
This means that Dijkstra's algorithm is efficient with a simple binary heap.

To find the nearest neighbor, we use a precomputed k-d tree.
This results in very fast queries during runtime.
The construction algorithm is reminiscent of quicksort and can be parallelized efficiently.
On disk the k-d tree is stored as a permutation of vertex and step indices.

We use Go as our implementation language.
Go is a modern, concurrent and statically compiled programming language.
As the standard library comes with an HTTP server and JSON marshaling,
the initial implementation went very smoothly.

\section{Encountered Problems}

The OSM data turned out to be astonishingly clean, compared to the output of a monkey with a typewriter.
By default the data is sorted by creation date, which is nice if you want to build a history viewer.
For our purposes, this ordering is useless if you want to apply any kind of graph algorithm
directly to the OSM files.

Profiling revealed that we had performance degradation due to too many memory allocations.
Thus, we improved the performance by replacing our "reference" structs for vertices and edges by integer indices.

Due to the Go garbage collector, the addressable memory on a 64 bit machine is limited to 16 GB.
At the moment our parser needs more memory to process the whole of Germany for pedestrian routing.
We intend to solve this by allocating large data structures in the non-GC heap
and reducing our memory footprint.
If it turns out that we have to swap data, we intend to use an off-the-shelf key-value store (e.g. LevelDB, RethinkDB).
But this is only for the pre-processing.
For the server, we try to avoid swapping at all costs.

\section{Additional Features}

Our server can perform concurrent non-blocking logging with accurate timing information.
In addition, the server can output profiling data for a request;
this has already been useful in troubleshooting performance problems.

Additionally, we have created a small JavaScript based frontend that 
gives us an easy way to test and visualize the results of our implementation
(integrated into our server).

\section{Future Work}

Even for a route request from Saarbrücken to Frankfurt Dijkstra's algorithm takes an inordinate amount of time (1 second).
Therefore, we are going to implement an algorithm based on hierarchical hub labeling~\cite{abra12}.

Furthermore, we have to ensure that the street graphs continue to fit into memory.
We are going to accomplish this by using succinct data structures;
Elias Fano encoding for vertex indices and delta compression for edges and positions~\cite{gon05, vig08}.

The quality of the input data is also an issue.
Currently there are multiple strongly connected components in the input data.
In the future we are going to partition the graph and throw away erroneous nodes.
Finding strongly connected components in large graphs is a difficult problem.
However, since we know that the graph contains a large SCC,
a simple randomized strategy can be used~\cite{don07}.

To improve the compression and locality, we intend to use a locality preserving ordering based on space filling curves
(e.g. Hilbert curves~\cite{moo08}).

The distance metric can be computed directly and this is already implemented.
In order to compute fastest paths we need estimates of the average traveling speed.
This topic needs additional research.
Existing routing services use a combination of different heuristics.
For supporting energy efficient routing we need precise elevation data.
Fortunately, topographical surveys are readily available, see e.g. \cite{farr07}.
In the future we will include the SRTM data set during pre-processing to compute
more accurate distances.

\begin{thebibliography}{9}

\bibitem{abra12}
  I.~Abraham, D.~Delling, A.~V.~Goldberg, R.~F.~Werneck,
  \emph{Hierarchical Hub Labelings for Shortest Paths}.
  Technical Report,
  Microsoft Research,
  2012,
  http://research.microsoft.com/pubs/163231/MSR-TR-2012-46.pdf.

\bibitem{don07}
	D. Donato, L. Laura, S. Leonardi, and S. Millozzi, \emph{The Web as a graph: How far we are}. ACM Trans. Internet Technol. 7, 1, Article 4, 2007. 

\bibitem{farr07}
	T. G. Farr, et al., \emph{The Shuttle Radar Topography Mission}. Rev. Geophys., 45, RG2004, 2007, doi:10.1029/2005RG000183.
	
\bibitem{gon05}
	R. González, S. Grabowski, V. Mäkinen, and G. Navarro, \emph{Practical implementation of rank and select queries}. In Poster Proceedings Volume of 4th Workshop on Efficient and Experimental Algorithms (WEA'05), 2005.
	
\bibitem{moo08}
	D. Moore, \emph{Fast Hilbert Curve Generation, Sorting, and Range Queries}. 2008, http://www.tiac.net/~sw/2008/10/Hilbert/moore/.
	
\bibitem{vig08}
	S. Vigna, \emph{Broadword Implementation of Rank/Select Queries}. Experimental Algorithms, Springer Berlin / Heidelberg, 2008.

\end{thebibliography}

\end{document}
